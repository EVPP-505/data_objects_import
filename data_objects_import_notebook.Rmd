---
title: "Data Frames and Data Import"
output: html_notebook
---

The data frame is a culmination of all the base structures we have been working on.  A data frame is a collection of vectors presented in a single file that can be indexed and operated on in the same manner as vectors.  Data frames can also be manipulated using various new tools that will be introduced along the way.

Unlike an Excel spreadsheet, an R data frame imposes the same basic properties as a vector on each column of data (because they are vectors!).  That means each column can only be a single data type.  Also, all columns in a data frame must be of **equal length**, though you can add N/A's to make up the difference for missing observations.

The code below will illustrate the base R mechanisms for extracting columns (vectors) of data and then further subsetting the data from a data frame.  In the process, we will learn to use the more modern, RStudio implementation of the data frame, called a `tibble`.

We will also use new packages supporting tibbles like `readr`, which implement existing import functions like `read.csv()` in a more modern (and usually faster) fashion using `read_csv()`.  You can read more about this in the help Vignettes (search `readr` in the help tab).

We will be using several R packages (libraries) from RStudio called tidyverse.  The basic syntax for loading a library is as follows.  **ALL** library calls should be placed at the top of you code (script, notebook, presentation, whatever you are using).  Any libraries you add in later analysis should also be added to the main library call.

```{r load-libraries}
# load the necessary libraries, one per line
library(dplyr)
library(readr)
library(tibble)

# alternately we can load a core subset of libraries, including the above.  
# Generally, we only call either the group above, or the one below, not both, as it is redundant
# library(tidyverse)

# ------------------

# Excel files require a separate library.  There are several to choose from, 
# but the RStudio library is clean and fast to use
library(readxl)
```


Let's start building a data frame using a set of vectors.

```{r vectors}
# a vector of integers
v1 <- c(1,2,3,4)

# a vector of strings
v2 <- c("a", "b", "c", "d")

# a vector of booleans
v3 <- c(TRUE, FALSE, FALSE, TRUE)

# a vector of floating points
v4 <- c(1.1, 1.2, 3.3)

```

Now, we will use the base R data frame constructor.  As mentioned earlier, a data frame requires all vectors to be equal length (unlike a spreadsheet).

```{r data-frame}
# create a data frame using base R.  The function call is simply a comma-separated list of vectors to combine.
# What happens when the next line of code is run?
df1 <- data.frame( v1, v2, v3, v4)

# try again with just the first three
df1 <- data.frame(v1, v2, v3)

# print to screen
df1

# now add a no-data value to the fourth vector to make it equal length
# NA (all caps, no quotes) is a reserved word
v4 <- c(v4, NA)

# try again
df1 <- data.frame( v1, v2, v3, v4)
df1
```

What is the data type of v2?  What were you expecting that to be?


Let's look at a modern Tibble

```{r tibble}
# now create a tibble using the tibble package (library)
df2 <- tibble(v1, v2, v3, v4)
# print to screen
df2
```

What type of variable is v2 now?  The `tibble` data frame no longer assumes strings are factors.  This was originally done to make the data frame more memory-efficient, which is no longer necessary.

A more complex [and ultimately more meaningful] variable assignment is made using a 'new_column_name' = variable syntax. You can change the names of columns; it is arbitrary, but make the new assigment names meaningful to you.

As a side note, writing clean, legible code is important.  R code interpreters, like most modern code interpreters, ignore whitespace and line breaks, making it possible to write code one line at a time.  Interpreters will look for **closures** ending statements.  Closures include `() and {}`.

```{r tibble-construction}
# a new data frame with meaningful column names.  
# Note, we can write each variable assignment on a separate line for clarity
df3 <- tibble(numbers = v1, # assign the integer to a named column
              abc = v2, # assign the strings
              logics = v3, # assign the logical
              # we can also assign new vectors an explicitly assign a variable type
              facts = as.factor( 
                c("a", "b", "b", "a")
                )
              )
df3
```

Indexing can also be performed on columns of the data frame.  You might start to see a special `[[` notation.  The double bracket is used for indexing lists.  In the case of a tibble, the main thing to note is that `[[` will extract a **vector** of data, instead of the full column.  We can also use the `$` notation to extract vectors of data by column name.  RStudio is smart enough to provide column names for a referenced data frame as a convenience.

Using a single bracket `[]` to extract a column will result in a new tibble.

```{r index-data-frame}
# extract a vector of data by position
df3[[2]] # the second column of data

# now extract the same vector using the column name
df3$abc

# extract the column as a new tibble
df3[2]

# we can also extract columns by name, but, again, we get a new tibble
df3["abc"]
```

We can also perform fancier indexing.  We can extract data in multiple dimensions.  R is row-major, meaning data are arranged as [row, column].  We extract data from a data frame (which is 2D) using those two dimensions.

```{r fancy-indexing}

# Extract columns 1, 3, 4 from the first row
df3[1 ,c(1,3,4)]

# we can also use conditions to extract subsets of data.
df3[df3$facts == "a" & df3$numbers > 2, ]
# the extra , above is required to extract all the columns.  We can alternately extract only a subset of columns.

# and further subset the data to only two columns in this case
df3[,3:4]

# combine the two examples
df3[df3$facts == "a" & df3$numbers > 2, 3:4]
```

We will learn new methods for extracting an subsetting data later.


## Data import

Most of the data we will use for this class will be flat files with two dimensions (row, columns), or in some cases 3-dimensions with row, column, and sheet.  To import these data base R has functions to read text-based files that are **delimited**.  Delimited files mean individual columns are separated by some machine-readable character.  The most common delimiters types are commas (,), semicolon (;), tabs, and whitespace (some regular number of spaces).

The base R functions are fine, but modern implementations are faster, so we will be using the `readr` package implementations to read files.  The basic function is called `read_table()`, which has arguments to provide different types of delimiters.  There is also a convenience function for comma-separated files that adds in default values for delimiters.

We talked about using the RStudio GUI interface to walk through importing data: File -> Import Dataset -> From text (readr).  However, that method is not reproducible; you can't share that workflow with a collaborator.  RStudio is really great a transparently showing the code generated by the GUI.  The following examples use the code from the GUI.

```{r read-csv}
# Read a common comma-separated file
# we assign the output of the file to a variable.  
# Remember to make your variable names meaningful and descriptive
# In this case the data are a "thing", so we make the variable a noun and use separators instead of spaces

# The function requires a single argument, the path to the file.  Other arguments are optional.
# The extra argument below is used to define the date column as a date type insted of a string.  
# readr is generally very good at figuring out data types, but dates are tricky and need to be defined manually.
# You can also force other columns in to a particular data type in advance.
# The statement using "col_types" is a named argument to which we pass the name of the columns to parse
NEON_D02_SCBI_1m2Data <- read_csv("data/scbi/inventory/NEON.D02.SCBI.DP1.10058.001.div_1m2Data.csv", 
                                  # cols() is a function that accepts a list of column names to search and
                                  # the required format
                                  col_types = cols(
                                                date = col_date(format = "%Y-%m-%d")
                                                )
                                  )
# print the result to screen
# Note the date field is now a formatted date
NEON_D02_SCBI_1m2Data
```

The next example illustrates the more generic `read_table()` function to read a file that is not as cleanly formatted.  There are several lines of text at the beginning of the file that describe the columns of data.  This is not an uncommon way to produce data.

R will default to use the first line of data as a header unless told otherwise.  If we do nothing more than say not to treat the first line as a header using, `col_names = FALSE`, R will make up generic names like X1...Xn.  

```{r read-txt}

scbi_grove_species_summaries_neon <- read_table("data/scbi_grove_species_summaries_neon.txt",
                                              col_names = FALSE,  skip = 8)

# optionally view the results in a spreadsheet-like view
# View(scbi_grove_species_summaries_neon)

# print to screen
scbi_grove_species_summaries_neon
```

For this file, we will tell R to skip the extra lines and add a vector of names to use.

```{r read-txt-names}
scbi_grove_species_summaries_neon <- read_table("data/scbi_grove_species_summaries_neon.txt",
    col_names = c("wavelength", "mean_sp1", "mean_sp2", "mean_sp3", "mean_sp4", "mean_sp5", "mean_sp6"),
    skip = 8)
# View(scbi_grove_species_summaries_neon)
scbi_grove_species_summaries_neon

```

Excel files are also a common way to share data.  However, the Excel file format is proprietary, and should generally be avoided in favor of text files; export each the sheets to a separate, delimited file for sharing.  Excel files are also a binary type, which excludes it from change tracking like Git.  Regardless, Excel can easily be imported using R, but it requires a different library, `readxl`.

`read_excel` requires a file name like the previous text import examples.  Like the `readr` library, `readxl` allows for parsing and defining columns in the same way, using the same arguments.  The primary difference is the added argument for telling the function which sheet to import, since R can only read one sheet at a time.  The function will default to reading the first sheet.

```{r read_xl}
# read the first sheet of an Excel file
Filodata_processing <- read_excel("data/filo_data/Filodata_processing.xlsx", 
    sheet = "Sheet1")
# View(Filodata_processing)

# print to screen
Filodata_processing
```


## Filters and Indexes

Introducing the dplyr library  You can find the cheatsheet using the Help -> Cheatsheets -> Data Manipulation with `dplyr`, tiydr or [This link](https://www.rstudio.com/resources/cheatsheets/)

The library introduces functions for achieveing the same type of indexing in the code above using a more friendly vernacular.  Instead of using [] and $ notation, `dplyr` uses functions (verbs) like `select()` and `filter()` to achieve the same result in what is meant to be a user-friendy, approachable fashion.

`dyplr` also introduces chaining operations, or putting togther multiple operations into a single, cohesive line of code, using the pipe operator, `%>%`.  There is no functional limit to the number of operations you can chain together, but functionally, operations should be grouped meaningfully.

```{r filter-data}
# The call to dplyr starts with the object (data frame), we are operating on followed by the %>% operator.  
# All decendant operations assume we are using the same data.
# Note: order of operations are important, starting from the first line down.
NEON_D02_SCBI_1m2Data %>%
  filter(taxonRank == "genus" & percentCover <= 0.5) %>% # extract only the rows (observations) we are interested in
  select(one_of(c("scientificName", "decimalLatitude", "decimalLongitude", "taxonRank"))) # subset the result to only a small group of columns
```

In this last example, we were able to accompish the same result in a chain of code that is [perhaps arguably] more readable, and thus reproducable, than base R.  However, the same can be accomplished using base R in a single line, though this is more cryptic if you are not familiar with the notation.


```{r base-r-filter}

NEON_D02_SCBI_1m2Data[NEON_D02_SCBI_1m2Data$taxonRank == "genus" & NEON_D02_SCBI_1m2Data$percentCover <= 0.5, c(15, 6, 7, 16)]

```

The cardinal rule of data analysis is the original data is immutable and sacred.  This means your source data remains intact and unchanged for reproducibility.  That is, all derivative analysis should be kept separate from the source data.   So, the last thing to do is export the result to a new file.  The output of results should be stored separately from the source material to avoid confusion (e.g. using a "results" folder).

The `readr` package includes functions for just this purpose.  The nice thing about the `tidyverse` is that all the various package functions can be chained together to form a single string of operations on a data set, including saving the output.  Using the `dplyr` flow above, we can add a new line to output the results to a new file.

```{r export-results, eval=FALSE}
NEON_D02_SCBI_1m2Data %>%
  filter(taxonRank == "genus" & percentCover <= 0.5) %>% # extract only the rows (observations) we are interested in
  select(one_of(c("scientificName", "decimalLatitude", "decimalLongitude", "taxonRank"))) %>% # subset the result to only a small group of columns
  write_csv("results/filter_example.csv")
```









